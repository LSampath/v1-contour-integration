{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brian2\n",
    "from brian2tools import *\n",
    "from brian2 import *\n",
    "from struct import unpack\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.cm as cmap\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "import pickle as pickle\n",
    "import scipy\n",
    "import scipy.signal as signal\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix_from_file(fileName, shape):\n",
    "    readout = np.load(fileName)\n",
    "#     print(readout.shape, fileName)\n",
    "    value_arr = np.zeros(shape)\n",
    "    if not readout.shape == (0,):\n",
    "        value_arr[np.int32(readout[:,0]), np.int32(readout[:,1])] = readout[:,2]\n",
    "    return value_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian2D(x, y, sigma):\n",
    "    return (1.0/(1*math.pi*(sigma**2)))*math.exp(-(1.0/(sigma**2))*(x**2 + y**2))\n",
    "\n",
    "def mexicanHat(x,y,sigma1,sigma2): \n",
    "    return gaussian2D(x,y,sigma1) - gaussian2D(x,y,sigma2)\n",
    "\n",
    "def receptiveFieldMatrix(func):\n",
    "    h = 5\n",
    "    g = np.zeros((h,h))\n",
    "    for xi in range(0,h):\n",
    "        for yi in range(0,h):\n",
    "            x = xi-int(h/2)\n",
    "            y = yi-int(h/2)\n",
    "            g[xi, yi] = func(x,y);\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_stimuli(data, r, x):\n",
    "    data = (data - data.min()) / (data.max() - data.min())\n",
    "    q95 = np.percentile(data, 95)\n",
    "    data = np.where((data > q95), q95, data)\n",
    "    q50 = np.percentile(data, 50)\n",
    "    data = np.where((data < q50), q50, data)\n",
    "    if (r!=0):\n",
    "        data = np.exp(data*r)\n",
    "    data = (data - data.min()) / (data.max() - data.min())\n",
    "    data *= x\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stimuli(file_name, r1, r2, x):\n",
    "    img = Image.open(file_name)\n",
    "    img.load()\n",
    "    img_arr = np.asarray(img, dtype=\"int32\")\n",
    "    stimuli = np.absolute(img_arr)/255\n",
    "    img.close()\n",
    "    print(stimuli.shape)\n",
    "    \n",
    "    stimuli_on = signal.convolve(stimuli, receptiveFieldMatrix(lambda x,y:mexicanHat(x,y,1,1.1)), mode='same')\n",
    "    stimuli_on = tune_stimuli(stimuli_on, r1, x)\n",
    "    stimuli_off = signal.convolve(stimuli, receptiveFieldMatrix(lambda x,y:mexicanHat(x,y,1.1,1)), mode='same')\n",
    "    stimuli_off = tune_stimuli(stimuli_off, r2, x)\n",
    "\n",
    "    c, axarr = subplots(1, 3, figsize = (15, 6))\n",
    "    axarr[0].imshow(stimuli, cmap = cmap.get_cmap('binary'))\n",
    "    axarr[0].title.set_text('stimuli')\n",
    "    axarr[1].imshow(stimuli_on, cmap = cmap.get_cmap('binary'))\n",
    "    axarr[1].title.set_text('stimuli_on')\n",
    "    axarr[2].imshow(stimuli_off, cmap = cmap.get_cmap('binary'))\n",
    "    axarr[2].title.set_text('stimuli_off')\n",
    "    \n",
    "    return stimuli_on, stimuli_off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_connections():\n",
    "    for connName in connections:\n",
    "        conn = connections[connName]\n",
    "        weights = np.column_stack((conn.i, conn.j, conn.w))\n",
    "        sparseWeights = weights[~(weights.transpose()[2] == 0)]\n",
    "        np.save(saved_path + connName, sparseWeights)\n",
    "        print(connName, end=' ')\n",
    "    print('connections saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_surrounding(coordinate, r, inner, length):\n",
    "    y = coordinate[0]\n",
    "    x = coordinate[1]\n",
    "    \n",
    "    x_min = 0 if (x-r < 0) else (x-r)\n",
    "    x_max = (length-1) if (x+r > length-1) else (x+r)\n",
    "    y_min = 0 if (y-r < 0) else (y-r)\n",
    "    y_max = (length-1) if (y+r > length-1) else (y+r)\n",
    "    \n",
    "    coordinates = []\n",
    "    for i in range(y_min, y_max+1):\n",
    "        for j in range(x_min, x_max+1):\n",
    "            if (i==y and j==x and not inner):\n",
    "                continue\n",
    "            coordinates.append((i,j))\n",
    "            \n",
    "    return coordinates\n",
    "\n",
    "def get_prefered_surrounding(coordinate, o_index, c_length):\n",
    "    x = coordinate[0]\n",
    "    y = coordinate[1]\n",
    "    \n",
    "    arr = []\n",
    "    if (o_index == h_index):\n",
    "        arr = [(x,y-2), (x,y-1), (x,y+1), (x,y+2)]\n",
    "    elif (o_index == v_index):\n",
    "        arr = [(x-2,y), (x-1,y), (x+1,y), (x+2,y)]\n",
    "    elif (o_index == d1_index):\n",
    "        arr = [(x+2,y-2), (x+1,y-1), (x-1,y+1), (x-2,y+2)]\n",
    "    elif (o_index == d2_index):\n",
    "        arr = [(x+2,y+2), (x+1,y+1), (x-1,y-1), (x-2,y-2)]\n",
    "    \n",
    "    coordinates = []\n",
    "    for (p,q) in arr:\n",
    "        if ((p>-1 and p<c_length) and (q>-1 and q<c_length)):\n",
    "            coordinates.append((p,q))\n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lateral_distribution(name):\n",
    "    weights = np.array(connections[name].w)\n",
    "    weights = weights.reshape(shape)\n",
    "    \n",
    "    # init dictionary\n",
    "    list_dic = {}; sum_dic = {}; total_sum = 0\n",
    "    \n",
    "    c = {0:'h', 1:'v', 2:'d1', 3:'d2'}\n",
    "    for s_index in range(orientations):\n",
    "        for t_index in range(orientations):\n",
    "            list_dic[c[s_index]+c[t_index]] = []\n",
    "            sum_dic[c[s_index]+c[t_index]] = 0\n",
    "\n",
    "    for i in range(0, c_length):                   # y axis\n",
    "        for j in range(0, c_length):               # x axis\n",
    "            for p,q in get_surrounding((i,j), lateral_range, False, c_length):\n",
    "                \n",
    "                for s_index in range(orientations):\n",
    "                    for t_index in range(orientations):\n",
    "                        target = ( p * c_length + q ) * orientations + t_index\n",
    "                        source = ( i * c_length + j ) * orientations + s_index\n",
    "                        list_dic[c[s_index]+c[t_index]].append(weights[source][target])\n",
    "                        sum_dic[c[s_index]+c[t_index]] += abs(weights[source][target])\n",
    "                        total_sum += abs(weights[source][target])\n",
    "                        \n",
    "    category_names = ['H', 'V', 'Diagonal (45)', 'Diagonal (135)']\n",
    "    results = { 'h': [], 'v': [], 'd1': [], 'd2': [] }\n",
    "    %run horizontal_barchart_distribution.py\n",
    "                        \n",
    "    plt.figure(figsize=(17,4))\n",
    "    for s_index in range(orientations):\n",
    "        for t_index in range(orientations):\n",
    "            plt.plot(list_dic[c[s_index]+c[t_index]], label=c[s_index]+c[t_index])\n",
    "            \n",
    "            print(c[s_index]+c[t_index], sum_dic[c[s_index]+c[t_index]], \n",
    "                  'of', total_sum, ':', sum_dic[c[s_index]+c[t_index]]*100/total_sum)\n",
    "            \n",
    "            results[c[s_index]].append(sum_dic[c[s_index]+c[t_index]])\n",
    "    \n",
    "    plt.legend(); plt.show()\n",
    "    survey(results, category_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_preferred_distribution():\n",
    "    c = {0:'horizontal', 1:'vertical', 2:'diagonal 45', 3:'diagonal 135'}\n",
    "    shape = (n_L3, n_L3)\n",
    "    weights = np.array(connections['L3_L3'].w)\n",
    "    weights = weights.reshape(shape)\n",
    "    \n",
    "    # init dictionary\n",
    "    w_arr = [[], [], [], []]\n",
    "    \n",
    "    for i in range(0, c_length):                   # y axis\n",
    "        for j in range(0, c_length):               # x axis\n",
    "            for o_index in range(orientations):\n",
    "                for p,q in get_prefered_surrounding((i,j), o_index, c_length):\n",
    "                    target = ( p * c_length + q ) * orientations + o_index\n",
    "                    source = ( i * c_length + j ) * orientations + o_index\n",
    "                    w_arr[o_index].append(weights[source][target])\n",
    "                        \n",
    "    plt.figure(figsize=(17,12))\n",
    "    for o_index in range(orientations):\n",
    "        plt.subplot(4, 1, o_index+1)\n",
    "        plt.plot(w_arr[o_index], label=c[o_index])\n",
    "        plt.legend()\n",
    "        print(c[o_index], ' - ', sum(w_arr[o_index]))\n",
    "    \n",
    "    plt.show()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_integration(coordinates):\n",
    "    c = ['horizontal', 'vertical', 'diagonal 45', 'diagonal 135']\n",
    "    shape = (c_length, c_length, orientations)\n",
    "    \n",
    "    grid = np.array(spike_monitors['L3'].count).reshape(shape).transpose()\n",
    "    plt.figure(figsize=(15,2))\n",
    "    for o_index in range(orientations):\n",
    "        arr = []\n",
    "        for (i,j) in coordinates:\n",
    "            arr.append(grid[o_index].transpose()[i,j])\n",
    "        plt.plot(arr, label=c[o_index])\n",
    "    plt.legend(); plt.show()\n",
    "\n",
    "    grid = np.array(spike_monitors['L4'].count).reshape(shape).transpose()\n",
    "    plt.figure(figsize=(15,2))\n",
    "    for o_index in range(orientations):\n",
    "        arr = []\n",
    "        for (i,j) in coordinates:\n",
    "            arr.append(grid[o_index].transpose()[i,j])\n",
    "        plt.plot(arr, label=c[o_index])\n",
    "    plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIALIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli_file_path = './pic_44_2.tif'\n",
    "\n",
    "initial_path = './initial_weights/'\n",
    "saved_path = './saved_weights/'\n",
    "\n",
    "fig_num = 1\n",
    "\n",
    "field_size = 5                     # 2/3/4/5/6\n",
    "margin = (field_size//2)\n",
    "\n",
    "c_length = 20\n",
    "r_length = c_length*2 + margin*2\n",
    "orientations = 4\n",
    "\n",
    "h_index = 0\n",
    "v_index = 1\n",
    "d1_index = 2\n",
    "d2_index = 3\n",
    "\n",
    "n_LGN = r_length*r_length\n",
    "n_L4  = c_length*c_length*orientations\n",
    "n_L3  = n_L4\n",
    "n_L2  = n_L3\n",
    "\n",
    "lateral_range = 2\n",
    "\n",
    "num_epochs = 12\n",
    "\n",
    "single_example_time = 0.45 * second\n",
    "resting_time = 0.25 * second\n",
    "\n",
    "# delay = {}\n",
    "# delay = (0*ms, 5*ms)              # min and max delay\n",
    "# minDelay = delay[0]\n",
    "# maxDelay = delay[1]\n",
    "# deltaDelay = maxDelay - minDelay\n",
    "\n",
    "# neural model parameters\n",
    "v_rest_e = -65. * mV\n",
    "v_rest_i = -60. * mV\n",
    "v_reset_e = -65. * mV\n",
    "v_reset_i = -45. * mV\n",
    "v_thresh_e = -52 * mV\n",
    "v_thresh_i = -40. * mV\n",
    "refrac_e = 20. * ms\n",
    "refrac_i = 15. * ms\n",
    "tc_theta = 1e7 * ms\n",
    "theta_plus_e = 0.2 * mV\n",
    "offset = 20.0 * mV\n",
    "\n",
    "# STDP parameters\n",
    "tc_pre = 50*ms\n",
    "tc_post = 40*ms\n",
    "nu_pre =  0.05# 0.0001\n",
    "nu_post = 0.1 # 0.01\n",
    "wmax = 100.0\n",
    "Apre = 0.4\n",
    "Apost = Apre*1.05\n",
    "\n",
    "input_intensity = 1.\n",
    "start_input_intensity = input_intensity\n",
    "\n",
    "#membrane dynamics\n",
    "scr_e = 'v = v_reset_e; theta += theta_plus_e; timer = 0*ms'\n",
    "v_reset_i_str = 'v = v_reset_i'\n",
    "\n",
    "v_thresh_e_str = '(v > (theta - offset + v_thresh_e)) and (timer > refrac_e)'\n",
    "v_thresh_i_str = 'v > v_thresh_i'\n",
    "\n",
    "neuron_eqs_e = '''\n",
    "        dv/dt = ((v_rest_e - v) + g_e*(-v) + g_i*(-100.*mV - v) ) / (100*ms)  : volt (unless refractory)\n",
    "        dg_e/dt = -g_e/(1.0*ms)                                    : 1\n",
    "        dg_i/dt = -g_i/(2.0*ms)                                    : 1\n",
    "        dtheta/dt = -theta/(1e7*ms)                                : volt\n",
    "        dtimer/dt = 0.1                                            : second\n",
    "'''\n",
    "\n",
    "neuron_eqs_i = '''\n",
    "        dv/dt = ((v_rest_i - v) +  g_e*(-v) + g_i*(-85.*mV - v)) / (10*ms)  : volt (unless refractory)\n",
    "        dg_e/dt = -g_e/(1.0*ms)                                    : 1\n",
    "        dg_i/dt = -g_i/(2.0*ms)                                    : 1\n",
    "'''\n",
    "\n",
    "# learning rules\n",
    "# without STDP\n",
    "model = 'w : 1'\n",
    "pre_e = 'g_e_post += w'\n",
    "pre_i = 'g_i_post += w'\n",
    "post = ''\n",
    "\n",
    "stdp_model = '''\n",
    "    post2_temp                          : 1\n",
    "    w                                   : 1\n",
    "    dpre/dt   =   -pre/(tc_pre)         : 1 (event-driven)\n",
    "    dpost/dt  =   -post/(tc_post)       : 1 (event-driven)\n",
    "'''\n",
    "stdp_pre = '''\n",
    "    w = clip(w + nu_pre*post, -wmax, wmax) * int(post>0.3*Apost) + clip(w - nu_pre*post, -wmax, wmax) * int(post<=0.3*Apost);\n",
    "    pre += Apre;\n",
    "'''\n",
    "stdp_pre_e = stdp_pre + 'g_e_post += w;'\n",
    "stdp_pre_i = stdp_pre + 'g_i_post += w;'\n",
    "stdp_post  = 'w = clip(w + nu_post * pre, 0, wmax); post += Apost;'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stimuli_on, stimuli_off = load_stimuli('./pic_60_1.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1498"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neuron_groups = {}\n",
    "neuron_groups_list = [\n",
    "    ('L4_i_NF', 'i'), ('L4_NF', 'e'), ('L4_i_FN', 'i'), ('L4_FN', 'e'),\n",
    "    ('L4', 'e'), \n",
    "    ('L3', 'e'), \n",
    "    ('L2', 'i')\n",
    "]\n",
    "\n",
    "for name, e_i in neuron_groups_list:\n",
    "    if (e_i == 'e'):\n",
    "        neuron_groups[name] = NeuronGroup(n_L4, neuron_eqs_e, threshold=v_thresh_e_str, refractory=refrac_e, reset=scr_e, method='euler')\n",
    "        neuron_groups[name].v    = v_rest_e - 40.*mV \n",
    "        neuron_groups[name].theta = np.ones((n_L4)) * 20.0*mV\n",
    "    elif (e_i == 'i'):\n",
    "        neuron_groups[name] = NeuronGroup(n_L4, neuron_eqs_i, threshold=v_thresh_i_str, refractory=refrac_i, reset=v_reset_i_str, method='euler')\n",
    "        neuron_groups[name].v    = v_rest_i - 40.*mV \n",
    "\n",
    "input_groups = {}\n",
    "input_groups['LGN_on']   = PoissonGroup(n_LGN, 0*Hz)\n",
    "input_groups['LGN_off']  = PoissonGroup(n_LGN, 0*Hz)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING    Cannot use Cython, a test compilation failed: Microsoft Visual C++ 14.0 is required. Get it with \"Microsoft Visual C++ Build Tools\": https://visualstudio.microsoft.com/downloads/ (DistutilsPlatformError) [brian2.codegen.runtime.cython_rt.cython_rt.failed_compile_test]\n",
      "INFO       Cannot use compiled code, falling back to the numpy code generation target. Note that this will likely be slower than using compiled code. Set the code generation to numpy manually to avoid this message:\n",
      "prefs.codegen.target = \"numpy\" [brian2.devices.device.codegen_fallback]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1816"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_path = initial_path\n",
    "# weight_path = saved_path\n",
    "connections = {}\n",
    "#[name, shape, weight_file, source, target, equation_type]\n",
    "\n",
    "input_neuron_conn = [\n",
    "    ['LGN_on_L4_NF', (n_LGN, n_L4), 'LGN_L4_NF.npy', 'LGN_on', 'L4_NF', 'e'],\n",
    "    ['LGN_off_L4_NF', (n_LGN, n_L4), 'LGN_L4_FN.npy', 'LGN_off', 'L4_NF', 'e'],\n",
    "    ['LGN_off_L4_i_NF', (n_LGN, n_L4), 'LGN_L4_NF.npy', 'LGN_off', 'L4_i_NF', 'e'],\n",
    "    ['LGN_on_L4_i_NF', (n_LGN, n_L4), 'LGN_L4_FN.npy', 'LGN_on', 'L4_i_NF', 'e'],\n",
    "    \n",
    "    ['LGN_off_L4_FN', (n_LGN, n_L4), 'LGN_L4_NF.npy', 'LGN_off', 'L4_FN', 'e'],\n",
    "    ['LGN_on_L4_FN', (n_LGN, n_L4), 'LGN_L4_FN.npy', 'LGN_on', 'L4_FN', 'e'],\n",
    "    ['LGN_on_L4_i_FN', (n_LGN, n_L4), 'LGN_L4_NF.npy', 'LGN_on', 'L4_i_FN', 'e'],\n",
    "    ['LGN_off_L4_i_FN', (n_LGN, n_L4), 'LGN_L4_FN.npy', 'LGN_off', 'L4_i_FN', 'e'],\n",
    "]\n",
    "\n",
    "neuron_neuron_conn = [\n",
    "    ['L4_i_L4_NF', (n_L4, n_L4), 'L4_L4.npy', 'L4_i_NF', 'L4_NF', 'i', False],\n",
    "    ['L4_i_L4_FN', (n_L4, n_L4), 'L4_L4.npy', 'L4_i_FN', 'L4_FN', 'i', False],\n",
    "    ['L4_NF_L4', (n_L4, n_L4), 'L4_L4.npy', 'L4_NF', 'L4', 'e', False],\n",
    "    ['L4_FN_L4', (n_L4, n_L4), 'L4_L4.npy', 'L4_FN', 'L4', 'e', False],\n",
    "    \n",
    "    ['L4_L3', (n_L4, n_L3), 'L4_L3.npy', 'L4', 'L3', 'e', False],\n",
    "    ['L3_L3', (n_L3, n_L3), 'L3_L3.npy', 'L3', 'L3', 'e', True],\n",
    "    ['L3_L2', (n_L3, n_L2), 'L3_L2.npy', 'L3', 'L2', 'e', False],\n",
    "    ['L2_L3', (n_L2, n_L3), 'L2_L3.npy', 'L2', 'L3', 'i', False],\n",
    "    \n",
    "    ['L3_L4', (n_L3, n_L4), 'L3_L4.npy', 'L3', 'L4', 'e', False]\n",
    "]\n",
    "\n",
    "\n",
    "for name, shape, weight_file, source, target, _ in input_neuron_conn:\n",
    "    weightMatrix = get_matrix_from_file(weight_path + weight_file, shape)\n",
    "    connections[name]= Synapses(input_groups[source], neuron_groups[target], model=model, on_pre=pre_e, on_post=post)\n",
    "    connections[name].connect(True)\n",
    "    connections[name].w = weightMatrix[connections[name].i, connections[name].j]\n",
    "#     connections[name].delay = 'minDelay + rand() * deltaDelay'\n",
    "    \n",
    "    \n",
    "for name, shape, weight_file, source, target, equation, learn in neuron_neuron_conn:\n",
    "    if (learn):\n",
    "        model_eq = stdp_model\n",
    "        post_eq = stdp_post\n",
    "        if (equation == 'e'):\n",
    "            pre_eq = stdp_pre_e\n",
    "        elif (equation == 'i'):\n",
    "            pre_eq = stdp_pre_i\n",
    "    else:\n",
    "        model_eq = model\n",
    "        post_eq = post\n",
    "        if (equation == 'e'):\n",
    "            pre_eq = pre_e\n",
    "        elif (equation == 'i'):\n",
    "            pre_eq = pre_i\n",
    "    \n",
    "    weightMatrix = get_matrix_from_file(weight_path + weight_file, shape)\n",
    "    connections[name]= Synapses(neuron_groups[source], neuron_groups[target], model=model_eq, on_pre=pre_eq, on_post=post_eq)\n",
    "    connections[name].connect(True)\n",
    "    connections[name].w = weightMatrix[connections[name].i, connections[name].j]\n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_monitors = {}\n",
    "\n",
    "spike_monitors['LGN_on']      = SpikeMonitor(input_groups['LGN_on'])\n",
    "spike_monitors['LGN_off']     = SpikeMonitor(input_groups['LGN_off'])\n",
    "\n",
    "for name,_ in neuron_groups_list:\n",
    "    spike_monitors[name]   = SpikeMonitor(neuron_groups[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network()\n",
    "for obj_list in [neuron_groups, input_groups, connections, spike_monitors]:\n",
    "    for key in obj_list:\n",
    "        net.add(obj_list[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hh 17907.516484911746 of 100226.45430962848 : 17.86705576712337\n",
      "hv 2573.3042967182314 of 100226.45430962848 : 2.567490104726793\n",
      "hd1 2572.821246114551 of 100226.45430962848 : 2.5670081455404605\n",
      "hd2 2573.2572132577184 of 100226.45430962848 : 2.5674431276478993\n",
      "vh 2573.249996765746 of 100226.45430962848 : 2.5674359274610605\n",
      "vv 17916.2149873062 of 100226.45430962848 : 17.875734615890767\n",
      "vd1 2573.301424170143 of 100226.45430962848 : 2.567487238669016\n",
      "vd2 2572.9276032635903 of 100226.45430962848 : 2.567114262383336\n",
      "d1h 2572.7545500989036 of 100226.45430962848 : 2.56694160021956\n",
      "d1v 2572.7912555331986 of 100226.45430962848 : 2.5669782227206235\n",
      "d1d1 16760.36879939859 of 100226.45430962848 : 16.72249997752187\n",
      "d1d2 2573.0884863048345 of 100226.45430962848 : 2.567274781921169\n",
      "d2h 2572.671459533175 of 100226.45430962848 : 2.56685869739086\n",
      "d2v 2573.4665332263776 of 100226.45430962848 : 2.5676519746734687\n",
      "d2d1 2572.7798938248834 of 100226.45430962848 : 2.5669668866832533\n",
      "d2d2 16765.940079199216 of 100226.45430962848 : 16.72805866942512\n"
     ]
    }
   ],
   "source": [
    "plot_lateral_distribution('L3_L3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_preferred_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stimuli files\n",
    "stimuli_on, stimuli_off = get_stimuli(stimuli_file_path, 5, 20, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "previous_spike_count = np.zeros(n_L4)\n",
    "net.run(resting_time)\n",
    "\n",
    "j = 0\n",
    "while j < (num_epochs):\n",
    "    spike_rates_on = stimuli_on.reshape((n_LGN)) / 8. * input_intensity\n",
    "    spike_rates_off = stimuli_off.reshape((n_LGN)) / 8. * input_intensity\n",
    "\n",
    "    input_groups['LGN_on'].rates = spike_rates_on * Hz\n",
    "    input_groups['LGN_off'].rates = spike_rates_off * Hz\n",
    "\n",
    "    print('run example number:', j+1, 'of', num_epochs)\n",
    "    net.run(single_example_time, report='text')   # 0.35 s\n",
    "\n",
    "    current_spike_count = np.asarray(spike_monitors['L4'].count[:]) - previous_spike_count\n",
    "    previous_spike_count = np.copy(spike_monitors['L4'].count[:])\n",
    "\n",
    "    if np.sum(current_spike_count) < 1:\n",
    "        if (input_intensity == 5):\n",
    "            break;\n",
    "        print(\"F - spike count\", np.sum(current_spike_count))\n",
    "        input_intensity += 1 \n",
    "    else:     \n",
    "        print(\"S - spike count\", np.sum(current_spike_count))\n",
    "        input_intensity = start_input_intensity\n",
    "        j += 1\n",
    "        \n",
    "    input_groups['LGN_on'].rates = 0 * Hz\n",
    "    input_groups['LGN_off'].rates = 0 * Hz\n",
    "    net.run(resting_time)\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_shape = (c_length, c_length, orientations)\n",
    "r_shape = (r_length, r_length)\n",
    "fig, axarr = subplots(2,5, figsize = (15, 7))\n",
    "\n",
    "data = np.copy(spike_monitors['L3'].count).reshape(c_shape).transpose()\n",
    "# q80 = np.percentile(data, 50)\n",
    "# data = np.where((data < q80), 0, data)\n",
    "\n",
    "v_min = data.min(); v_max = data.max()\n",
    "print('L3 v_min', v_min, 'v_max', v_max)\n",
    "\n",
    "im3 = axarr[0,0].imshow(data[h_index].transpose(), vmax=v_max, vmin=v_min, cmap = cmap.get_cmap('binary'))\n",
    "axarr[0,0].title.set_text('L3_h'); \n",
    "plt.colorbar(im3, ax=axarr[0,0])\n",
    "im3 = axarr[1,0].imshow(data[v_index].transpose(), vmax=v_max, vmin=v_min, cmap = cmap.get_cmap('binary'))\n",
    "axarr[1,0].title.set_text('L3_v'); \n",
    "plt.colorbar(im3, ax=axarr[1,0])\n",
    "im3 = axarr[0,1].imshow(data[d1_index].transpose(), vmax=v_max, vmin=v_min, cmap = cmap.get_cmap('binary'))\n",
    "axarr[0,1].title.set_text('L3_d1');\n",
    "plt.colorbar(im3, ax=axarr[0,1])\n",
    "im3 = axarr[1,1].imshow(data[d2_index].transpose(), vmax=v_max, vmin=v_min, cmap = cmap.get_cmap('binary'))\n",
    "axarr[1,1].title.set_text('L3_d2');\n",
    "plt.colorbar(im3, ax=axarr[1,1])\n",
    "\n",
    "\n",
    "data = np.copy(spike_monitors['L4'].count).reshape(c_shape).transpose()\n",
    "# q80 = np.percentile(data, 50)\n",
    "# data = np.where((data < q80), 0, data)\n",
    "\n",
    "v_min = data.min(); v_max = data.max()\n",
    "print('L4 v_min', v_min, 'v_max', v_max)\n",
    "\n",
    "im4 = axarr[0,2].imshow(data[h_index].transpose(), vmax=v_max, vmin=v_min, cmap = cmap.get_cmap('binary'))\n",
    "axarr[0,2].title.set_text('L4_h')\n",
    "plt.colorbar(im4, ax=axarr[0,2])\n",
    "im4 = axarr[1,2].imshow(data[v_index].transpose(), vmax=v_max, vmin=v_min, cmap = cmap.get_cmap('binary'))\n",
    "axarr[1,2].title.set_text('L4_v')\n",
    "plt.colorbar(im4, ax=axarr[1,2])\n",
    "im4 = axarr[0,3].imshow(data[d1_index].transpose(), vmax=v_max, vmin=v_min, cmap = cmap.get_cmap('binary'))\n",
    "axarr[0,3].title.set_text('L4_d1')\n",
    "plt.colorbar(im4, ax=axarr[0,3])\n",
    "im4 = axarr[1,3].imshow(data[d2_index].transpose(), vmax=v_max, vmin=v_min, cmap = cmap.get_cmap('binary'))\n",
    "axarr[1,3].title.set_text('L4_d2')\n",
    "plt.colorbar(im4, ax=axarr[1,3])\n",
    "\n",
    "\n",
    "data = np.copy(spike_monitors['LGN_on'].count).reshape(r_shape)\n",
    "v_min = data.min(); v_max = data.max()\n",
    "print('LGN on', 'max', v_max, 'min', v_min)\n",
    "axarr[0,4].imshow(data, vmax=v_max, vmin=v_min, cmap = cmap.get_cmap('binary'))\n",
    "axarr[0,4].title.set_text('LGN_on')\n",
    "\n",
    "data = np.copy(spike_monitors['LGN_off'].count).reshape(r_shape)\n",
    "v_min = data.min(); v_max = data.max()\n",
    "print('LGN off', 'max', v_max, 'min', v_min)\n",
    "axarr[1,4].imshow(data, vmax=v_max, vmin=v_min, cmap = cmap.get_cmap('binary'))\n",
    "axarr[1,4].title.set_text('LGN_off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarr = subplots(1,2, figsize = (12,6))\n",
    "\n",
    "d_L3 = np.copy(spike_monitors['L3'].count).reshape(c_shape).transpose()\n",
    "# q80 = np.percentile(d_L3, 95)\n",
    "# d_L3 = np.where((d_L3 < q80), 0, d_L3)\n",
    "\n",
    "d_L4 = np.copy(spike_monitors['L4'].count).reshape(c_shape).transpose()\n",
    "# q80 = np.percentile(d_L4, 95)\n",
    "# d_L4 = np.where((d_L4 < q80), 0, d_L4)\n",
    "\n",
    "f_L3 = np.zeros((c_length,c_length))\n",
    "f_L4 = np.zeros((c_length,c_length))\n",
    "for o_index in range(orientations):\n",
    "    f_L3 += d_L3[o_index].transpose()\n",
    "    f_L4 += d_L4[o_index].transpose()\n",
    "\n",
    "im_L3 = axarr[0].imshow(f_L3, cmap = cmap.get_cmap('binary'))\n",
    "axarr[0].title.set_text('L3');\n",
    "im_L4 = axarr[1].imshow(f_L4, cmap = cmap.get_cmap('binary'))\n",
    "axarr[1].title.set_text('L4');\n",
    "\n",
    "print('L4', d_L4.max())\n",
    "print('L3', d_L3.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bend_hv = [(7,1), (6,1), (5,1), (4,1), (3,1), (2,1), (1,1), (1,2), (1,3), (1,4), (1,5), (1,6), (1,7)]\n",
    "plot_integration(bend_hv)\n",
    "\n",
    "bend_h = [(1,4), (1,5), (1,6), (1,7), (1,8), (1,9), (1,10), (1,11), (1,12), (1,13), (1,14), (1,15)]\n",
    "plot_integration(bend_h)\n",
    "\n",
    "bend_v = [(5,18), (6,18), (7,18), (8,18), (9,18), (10,18), (11,18), (12,18), (13,18), (14,18)]\n",
    "plot_integration(bend_v)\n",
    "\n",
    "bend_dh = [(14,6), (15,7), (16,8), (17,9), (18,10), (18,11), (18,12), (18,13), (18,14), (18,15)]\n",
    "plot_integration(bend_dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_integration(18, v_index)\n",
    "# plot_integration(1, v_index)\n",
    "# plot_integration(18, h_index)\n",
    "# plot_integration(1, h_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_lateral_distribution('L3_L3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save connections\n",
    "# save_connections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot(spike_monitors['L4'].t/ms, spike_monitors['L4'].i, '.k')\n",
    "# xlabel('Time (ms)')\n",
    "# ylabel('Neuron index');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot(spike_monitors['L3'].t/ms, spike_monitors['L3'].i, '.k')\n",
    "# xlabel('Time (ms)')\n",
    "# ylabel('Neuron index');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot(spike_monitors['L2'].t/ms, spike_monitors['L2'].i, '.k')\n",
    "# xlabel('Time (ms)')\n",
    "# ylabel('Neuron index');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_preferred_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_matrix_from_file('./initial_weights/LGN_L4_NF.npy', (n_LGN, n_L4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNN simulator",
   "language": "python",
   "name": "sim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
